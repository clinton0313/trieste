:py:mod:`trieste.models.gpflow`
===============================

.. py:module:: trieste.models.gpflow

.. autoapi-nested-parse::

   This package contains the primary interface for Gaussian process models. It also contains a
   number of :class:`TrainableProbabilisticModel` wrappers for GPflow-based models.



Submodules
----------
.. toctree::
   :titlesonly:
   :maxdepth: 1

   builders/index.rst
   optimizer/index.rst
   sampler/index.rst


Package Contents
----------------

.. py:function:: build_gpr(data: trieste.data.Dataset, search_space: trieste.space.SearchSpace, kernel_priors: bool = True, likelihood_variance: Optional[float] = None, trainable_likelihood: bool = False) -> gpflow.models.GPR

   Build a :class:`~gpflow.models.GPR` model with sensible initial parameters and
   priors. We use :class:`~gpflow.kernels.Matern52` kernel and
   :class:`~gpflow.mean_functions.Constant` mean function in the model. We found the default
   configuration used here to work well in most situation, but it should not be taken as a
   universally good solution.

   We set priors for kernel hyperparameters by default in order to stabilize model fitting. We
   found the priors below to be highly effective for objective functions defined over the unit
   hypercube. They do seem to work for other search space sizes, but we advise caution when using
   them in such search spaces. Using priors allows for using maximum a posteriori estimate of
   these kernel parameters during model fitting.

   Note that although we scale parameters as a function of the size of the search space, ideally
   inputs should be normalised to the unit hypercube before building a model.

   :param data: Dataset from the initial design, used for estimating the variance of observations.
   :param search_space: Search space for performing Bayesian optimization, used for scaling the
       parameters.
   :param kernel_priors: If set to `True` (default) priors are set for kernel parameters (variance
       and lengthscale).
   :param likelihood_variance: Likelihood (noise) variance parameter can be optionally set to a
       certain value. If left unspecified (default), the noise variance is set to maintain the
       signal to noise ratio of value given by ``SIGNAL_NOISE_RATIO_LIKELIHOOD``, where signal
       variance in the kernel is set to the empirical variance.
   :param trainable_likelihood: If set to `True` Gaussian likelihood parameter is set to
       non-trainable. By default set to `False`.
   :return: A :class:`~gpflow.models.GPR` model.


.. py:function:: build_sgpr(data: trieste.data.Dataset, search_space: trieste.space.SearchSpace, kernel_priors: bool = True, likelihood_variance: Optional[float] = None, trainable_likelihood: bool = False, num_inducing_points: Optional[int] = None, trainable_inducing_points: bool = False) -> gpflow.models.SGPR

   Build a :class:`~gpflow.models.SGPR` model with sensible initial parameters and
   priors. We use :class:`~gpflow.kernels.Matern52` kernel and
   :class:`~gpflow.mean_functions.Constant` mean function in the model. We found the default
   configuration used here to work well in most situation, but it should not be taken as a
   universally good solution.

   We set priors for kernel hyperparameters by default in order to stabilize model fitting. We
   found the priors below to be highly effective for objective functions defined over the unit
   hypercube. They do seem to work for other search space sizes, but we advise caution when using
   them in such search spaces. Using priors allows for using maximum a posteriori estimate of
   these kernel parameters during model fitting.

   For performance reasons number of inducing points should not be changed during Bayesian
   optimization. Hence, even if the initial dataset is smaller, we advise setting this to a higher
   number. By default inducing points are set to Sobol samples for the continuous search space,
   and simple random samples for discrete or mixed search spaces. This carries
   the risk that optimization gets stuck if they are not trainable, which calls for adaptive
   inducing point selection during the optimization. This functionality will be added to Trieste
   in future.

   Note that although we scale parameters as a function of the size of the search space, ideally
   inputs should be normalised to the unit hypercube before building a model.

   :param data: Dataset from the initial design, used for estimating the variance of observations.
   :param search_space: Search space for performing Bayesian optimization, used for scaling the
       parameters.
   :param kernel_priors: If set to `True` (default) priors are set for kernel parameters (variance
       and lengthscale).
   :param likelihood_variance: Likelihood (noise) variance parameter can be optionally set to a
       certain value. If left unspecified (default), the noise variance is set to maintain the
       signal to noise ratio of value given by ``SIGNAL_NOISE_RATIO_LIKELIHOOD``, where signal
       variance in the kernel is set to the empirical variance.
   :param trainable_likelihood: If set to `True` Gaussian likelihood parameter is set to
       be trainable. By default set to `False`.
   :param num_inducing_points: The number of inducing points can be optionally set to a
       certain value. If left unspecified (default), this number is set to either
       ``NUM_INDUCING_POINTS_PER_DIM``*dimensionality of the search space or value given by
       ``MAX_NUM_INDUCING_POINTS``, whichever is smaller.
   :param trainable_inducing_points: If set to `True` inducing points will be set to
       be trainable. This option should be used with caution. By default set to `False`.
   :return: An :class:`~gpflow.models.SGPR` model.


.. py:function:: build_svgp(data: trieste.data.Dataset, search_space: trieste.space.SearchSpace, classification: bool = False, kernel_priors: bool = True, likelihood_variance: Optional[float] = None, trainable_likelihood: bool = False, num_inducing_points: Optional[int] = None, trainable_inducing_points: bool = False) -> gpflow.models.SVGP

   Build a :class:`~gpflow.models.SVGP` model with sensible initial parameters and
   priors. Both regression and binary classification models are
   available. We use :class:`~gpflow.kernels.Matern52` kernel and
   :class:`~gpflow.mean_functions.Constant` mean function in the model. We found the default
   configuration used here to work well in most situation, but it should not be taken as a
   universally good solution.

   We set priors for kernel hyperparameters by default in order to stabilize model fitting. We
   found the priors below to be highly effective for objective functions defined over the unit
   hypercube. They do seem to work for other search space sizes, but we advise caution when using
   them in such search spaces. Using priors allows for using maximum a posteriori estimate of
   these kernel parameters during model fitting.

   For performance reasons number of inducing points should not be changed during Bayesian
   optimization. Hence, even if the initial dataset is smaller, we advise setting this to a higher
   number. By default inducing points are set to Sobol samples for the continuous search space,
   and simple random samples for discrete or mixed search spaces. This carries
   the risk that optimization gets stuck if they are not trainable, which calls for adaptive
   inducing point selection during the optimization. This functionality will be added to Trieste
   in future.

   Note that although we scale parameters as a function of the size of the search space, ideally
   inputs should be normalised to the unit hypercube before building a model.

   :param data: Dataset from the initial design, used for estimating the variance of observations.
   :param search_space: Search space for performing Bayesian optimization, used for scaling the
       parameters.
   :param classification: If a classification model is needed, this should be set to `True`, in
       which case a Bernoulli likelihood will be used. If a regression model is required, this
       should be set to `False` (default), in which case a Gaussian likelihood is used.
   :param kernel_priors: If set to `True` (default) priors are set for kernel parameters (variance
       and lengthscale).
   :param likelihood_variance: Likelihood (noise) variance parameter can be optionally set to a
       certain value. If left unspecified (default), the noise variance is set to maintain the
       signal to noise ratio of value given by ``SIGNAL_NOISE_RATIO_LIKELIHOOD``, where signal
       variance in the kernel is set to the empirical variance. This argument is ignored in the
       classification case.
   :param trainable_likelihood: If set to `True` likelihood parameter is set to
       be trainable. By default set to `False`. This argument is ignored in the classification
       case.
   :param num_inducing_points: The number of inducing points can be optionally set to a
       certain value. If left unspecified (default), this number is set to either
       ``NUM_INDUCING_POINTS_PER_DIM``*dimensionality of the search space or value given by
       ``MAX_NUM_INDUCING_POINTS``, whichever is smaller.
   :param trainable_inducing_points: If set to `True` inducing points will be set to
       be trainable. This option should be used with caution. By default set to `False`.
   :return: An :class:`~gpflow.models.SVGP` model.


.. py:function:: build_vgp_classifier(data: trieste.data.Dataset, search_space: trieste.space.SearchSpace, kernel_priors: bool = True, noise_free: bool = False, kernel_variance: Optional[float] = None) -> gpflow.models.VGP

   Build a :class:`~gpflow.models.VGP` binary classification model with sensible initial
   parameters and priors. We use :class:`~gpflow.kernels.Matern52` kernel and
   :class:`~gpflow.mean_functions.Constant` mean function in the model. We found the default
   configuration used here to work well in most situation, but it should not be taken as a
   universally good solution.

   We set priors for kernel hyperparameters by default in order to stabilize model fitting. We
   found the priors below to be highly effective for objective functions defined over the unit
   hypercube. They do seem to work for other search space sizes, but we advise caution when using
   them in such search spaces. Using priors allows for using maximum a posteriori estimate of
   these kernel parameters during model fitting. In the ``noise_free`` case we do not use prior
   for the kernel variance parameters.

   Note that although we scale parameters as a function of the size of the search space, ideally
   inputs should be normalised to the unit hypercube before building a model.

   :param data: Dataset from the initial design, used for estimating the variance of observations.
   :param search_space: Search space for performing Bayesian optimization, used for scaling the
       parameters.
   :param kernel_priors: If set to `True` (default) priors are set for kernel parameters (variance
       and lengthscale). In the ``noise_free`` case kernel variance prior is not set.
   :param noise_free: If  there is a prior information that the classification problem is a
       deterministic one, this should be set to `True` and kernel variance will be fixed to a
       higher default value ``CLASSIFICATION_KERNEL_VARIANCE_NOISE_FREE`` leading to sharper
       classification boundary. In this case prior for the kernel variance parameter is also not
       set. By default set to `False`.
   :param kernel_variance: Kernel variance parameter can be optionally set to a
       certain value. If left unspecified (default), the kernel variance is set to
       ``CLASSIFICATION_KERNEL_VARIANCE_NOISE_FREE`` in the ``noise_free`` case and to
       ``CLASSIFICATION_KERNEL_VARIANCE`` otherwise.
   :return: A :class:`~gpflow.models.VGP` model.


.. py:class:: GPflowPredictor(optimizer: Optimizer | None = None)

   Bases: :py:obj:`trieste.models.interfaces.SupportsPredictJoint`, :py:obj:`trieste.models.interfaces.SupportsGetKernel`, :py:obj:`trieste.models.interfaces.SupportsGetObservationNoise`, :py:obj:`trieste.models.interfaces.HasReparamSampler`, :py:obj:`abc.ABC`

   A trainable wrapper for a GPflow Gaussian process model.

   :param optimizer: The optimizer with which to train the model. Defaults to
       :class:`~trieste.models.optimizer.Optimizer` with :class:`~gpflow.optimizers.Scipy`.

   .. py:method:: optimizer(self) -> trieste.models.optimizer.Optimizer
      :property:

      The optimizer with which to train the model.


   .. py:method:: model(self) -> gpflow.models.GPModel
      :property:

      The underlying GPflow model.


   .. py:method:: predict(self, query_points: trieste.types.TensorType) -> tuple[trieste.types.TensorType, trieste.types.TensorType]

      Return the mean and variance of the independent marginal distributions at each point in
      ``query_points``.

      This is essentially a convenience method for :meth:`predict_joint`, where non-event
      dimensions of ``query_points`` are all interpreted as broadcasting dimensions instead of
      batch dimensions, and the covariance is squeezed to remove redundant nesting.

      :param query_points: The points at which to make predictions, of shape [..., D].
      :return: The mean and variance of the independent marginal distributions at each point in
          ``query_points``. For a predictive distribution with event shape E, the mean and
          variance will both have shape [...] + E.


   .. py:method:: predict_joint(self, query_points: trieste.types.TensorType) -> tuple[trieste.types.TensorType, trieste.types.TensorType]

      :param query_points: The points at which to make predictions, of shape [..., B, D].
      :return: The mean and covariance of the joint marginal distribution at each batch of points
          in ``query_points``. For a predictive distribution with event shape E, the mean will
          have shape [..., B] + E, and the covariance shape [...] + E + [B, B].


   .. py:method:: sample(self, query_points: trieste.types.TensorType, num_samples: int) -> trieste.types.TensorType

      Return ``num_samples`` samples from the independent marginal distributions at
      ``query_points``.

      :param query_points: The points at which to sample, with shape [..., N, D].
      :param num_samples: The number of samples at each point.
      :return: The samples. For a predictive distribution with event shape E, this has shape
          [..., S, N] + E, where S is the number of samples.


   .. py:method:: predict_y(self, query_points: trieste.types.TensorType) -> tuple[trieste.types.TensorType, trieste.types.TensorType]

      Return the mean and variance of the independent marginal distributions at each point in
      ``query_points`` for the observations, including noise contributions.

      Note that this is not supported by all models.

      :param query_points: The points at which to make predictions, of shape [..., D].
      :return: The mean and variance of the independent marginal distributions at each point in
          ``query_points``. For a predictive distribution with event shape E, the mean and
          variance will both have shape [...] + E.


   .. py:method:: get_kernel(self) -> gpflow.kernels.Kernel

      Return the kernel of the model.

      :return: The kernel.


   .. py:method:: get_observation_noise(self) -> trieste.types.TensorType

      Return the variance of observation noise for homoscedastic likelihoods.

      :return: The observation noise.
      :raise NotImplementedError: If the model does not have a homoscedastic likelihood.


   .. py:method:: optimize(self, dataset: trieste.data.Dataset) -> None

      Optimize the model with the specified `dataset`.

      :param dataset: The data with which to optimize the `model`.


   .. py:method:: log(self) -> None

      Log model-specific information at a given optimization step.


   .. py:method:: reparam_sampler(self, num_samples: int) -> trieste.models.interfaces.ReparametrizationSampler[GPflowPredictor]

      Return a reparametrization sampler providing `num_samples` samples.

      :return: The reparametrization sampler.



.. py:class:: GaussianProcessRegression(model: GPR | SGPR, optimizer: Optimizer | None = None, num_kernel_samples: int = 10, num_rff_features: int = 1000)

   Bases: :py:obj:`trieste.models.gpflow.interface.GPflowPredictor`, :py:obj:`trieste.models.interfaces.TrainableProbabilisticModel`, :py:obj:`trieste.models.interfaces.FastUpdateModel`, :py:obj:`trieste.models.gpflow.interface.SupportsCovarianceBetweenPoints`, :py:obj:`trieste.models.interfaces.SupportsInternalData`, :py:obj:`trieste.models.interfaces.HasTrajectorySampler`

   A :class:`TrainableProbabilisticModel` wrapper for a GPflow :class:`~gpflow.models.GPR`
   or :class:`~gpflow.models.SGPR`.

   :param model: The GPflow model to wrap.
   :param optimizer: The optimizer with which to train the model. Defaults to
       :class:`~trieste.models.optimizer.Optimizer` with :class:`~gpflow.optimizers.Scipy`.
   :param num_kernel_samples: Number of randomly sampled kernels (for each kernel parameter) to
       evaluate before beginning model optimization. Therefore, for a kernel with `p`
       (vector-valued) parameters, we evaluate `p * num_kernel_samples` kernels.
   :param num_rff_features: The number of random Foruier features used to approximate the
       kernel when calling :meth:`trajectory_sampler`. We use a default of 1000 as it
       typically perfoms well for a wide range of kernels. Note that very smooth
       kernels (e.g. RBF) can be well-approximated with fewer features.

   .. py:method:: model(self) -> GPR | SGPR
      :property:

      The underlying GPflow model.


   .. py:method:: update(self, dataset: trieste.data.Dataset) -> None

      Update the model given the specified ``dataset``. Does not train the model.

      :param dataset: The data with which to update the model.


   .. py:method:: covariance_between_points(self, query_points_1: trieste.types.TensorType, query_points_2: trieste.types.TensorType) -> trieste.types.TensorType

      Compute the posterior covariance between sets of query points.

      .. math:: \Sigma_{12} = K_{12} - K_{x1}(K_{xx} + \sigma^2 I)^{-1}K_{x2}

      Note that query_points_2 must be a rank 2 tensor, but query_points_1 can
      have leading dimensions.

      :param query_points_1: Set of query points with shape [..., N, D]
      :param query_points_2: Sets of query points with shape [M, D]
      :return: Covariance matrix between the sets of query points with shape [..., L, N, M]
          (L being the number of latent GPs = number of output dimensions)


   .. py:method:: optimize(self, dataset: trieste.data.Dataset) -> None

      Optimize the model with the specified `dataset`.

      For :class:`GaussianProcessRegression`, we (optionally) try multiple randomly sampled
      kernel parameter configurations as well as the configuration specified when initializing
      the kernel. The best configuration is used as the starting point for model optimization.

      For trainable parameters constrained to lie in a finite interval (through a sigmoid
      bijector), we begin model optimization from the best of a random sample from these
      parameters' acceptable domains.

      For trainable parameters without constraints but with priors, we begin model optimization
      from the best of a random sample from these parameters' priors.

      For trainable parameters with neither priors nor constraints, we begin optimization from
      their initial values.

      :param dataset: The data with which to optimize the `model`.


   .. py:method:: find_best_model_initialization(self, num_kernel_samples: int) -> None

      Test `num_kernel_samples` models with sampled kernel parameters. The model's kernel
      parameters are then set to the sample achieving maximal likelihood.

      :param num_kernel_samples: Number of randomly sampled kernels to evaluate.


   .. py:method:: trajectory_sampler(self) -> trieste.models.interfaces.TrajectorySampler[GaussianProcessRegression]

      Return a trajectory sampler. For :class:`GaussianProcessRegression`, we build
      trajectories using a random Fourier feature approximation.

      :return: The trajectory sampler.


   .. py:method:: get_internal_data(self) -> trieste.data.Dataset

      Return the model's training data.

      :return: The model's training data.


   .. py:method:: conditional_predict_f(self, query_points: trieste.types.TensorType, additional_data: trieste.data.Dataset) -> tuple[trieste.types.TensorType, trieste.types.TensorType]

      Returns the marginal GP distribution at query_points conditioned on both the model
      and some additional data, using exact formula. See :cite:`chevalier2014corrected`
      (eqs. 8-10) for details.

      :param query_points: Set of query points with shape [M, D]
      :param additional_data: Dataset with query_points with shape [..., N, D] and observations
               with shape [..., N, L]
      :return: mean_qp_new: predictive mean at query_points, with shape [..., M, L],
               and var_qp_new: predictive variance at query_points, with shape [..., M, L]


   .. py:method:: conditional_predict_joint(self, query_points: trieste.types.TensorType, additional_data: trieste.data.Dataset) -> tuple[trieste.types.TensorType, trieste.types.TensorType]

      Predicts the joint GP distribution at query_points conditioned on both the model
      and some additional data, using exact formula. See :cite:`chevalier2014corrected`
      (eqs. 8-10) for details.

      :param query_points: Set of query points with shape [M, D]
      :param additional_data: Dataset with query_points with shape [..., N, D] and observations
               with shape [..., N, L]
      :return: mean_qp_new: predictive mean at query_points, with shape [..., M, L],
               and cov_qp_new: predictive covariance between query_points, with shape
               [..., L, M, M]


   .. py:method:: conditional_predict_f_sample(self, query_points: trieste.types.TensorType, additional_data: trieste.data.Dataset, num_samples: int) -> trieste.types.TensorType

      Generates samples of the GP at query_points conditioned on both the model
      and some additional data.

      :param query_points: Set of query points with shape [M, D]
      :param additional_data: Dataset with query_points with shape [..., N, D] and observations
               with shape [..., N, L]
      :param num_samples: number of samples
      :return: samples of f at query points, with shape [..., num_samples, M, L]


   .. py:method:: conditional_predict_y(self, query_points: trieste.types.TensorType, additional_data: trieste.data.Dataset) -> tuple[trieste.types.TensorType, trieste.types.TensorType]

      Generates samples of y from the GP at query_points conditioned on both the model
      and some additional data.

      :param query_points: Set of query points with shape [M, D]
      :param additional_data: Dataset with query_points with shape [..., N, D] and observations
               with shape [..., N, L]
      :return: predictive variance at query_points, with shape [..., M, L],
               and predictive variance at query_points, with shape [..., M, L]



.. py:class:: SparseVariational(model: gpflow.models.SVGP, optimizer: Optimizer | None = None)

   Bases: :py:obj:`trieste.models.gpflow.interface.GPflowPredictor`, :py:obj:`trieste.models.interfaces.TrainableProbabilisticModel`, :py:obj:`trieste.models.gpflow.interface.SupportsCovarianceBetweenPoints`

   A :class:`TrainableProbabilisticModel` wrapper for a GPflow :class:`~gpflow.models.SVGP`.

   :param model: The underlying GPflow sparse variational model.
   :param optimizer: The optimizer with which to train the model. Defaults to
       :class:`~trieste.models.optimizer.BatchOptimizer` with :class:`~tf.optimizers.Adam` with
       batch size 100.

   .. py:method:: model(self) -> gpflow.models.SVGP
      :property:

      The underlying GPflow model.


   .. py:method:: update(self, dataset: trieste.data.Dataset) -> None

      Update the model given the specified ``dataset``. Does not train the model.

      :param dataset: The data with which to update the model.


   .. py:method:: covariance_between_points(self, query_points_1: trieste.types.TensorType, query_points_2: trieste.types.TensorType) -> trieste.types.TensorType

      Compute the posterior covariance between sets of query points.

      Note that query_points_2 must be a rank 2 tensor, but query_points_1 can
      have leading dimensions.

      :param query_points_1: Set of query points with shape [..., A, D]
      :param query_points_2: Sets of query points with shape [B, D]
      :return: Covariance matrix between the sets of query points with shape [..., L, A, B]
          (L being the number of latent GPs = number of output dimensions)



.. py:class:: VariationalGaussianProcess(model: gpflow.models.VGP, optimizer: Optimizer | None = None, use_natgrads: bool = False, natgrad_gamma: Optional[float] = None)

   Bases: :py:obj:`trieste.models.gpflow.interface.GPflowPredictor`, :py:obj:`trieste.models.interfaces.TrainableProbabilisticModel`, :py:obj:`trieste.models.interfaces.SupportsInternalData`, :py:obj:`trieste.models.gpflow.interface.SupportsCovarianceBetweenPoints`

   A :class:`TrainableProbabilisticModel` wrapper for a GPflow :class:`~gpflow.models.VGP`.

   A Variational Gaussian Process (VGP) approximates the posterior of a GP
   using the multivariate Gaussian closest to the posterior of the GP by minimizing the
   KL divergence between approximated and exact posteriors. See :cite:`opper2009variational`
   for details.

   The VGP provides (approximate) GP modelling under non-Gaussian likelihoods, for example
   when fitting a classification model over binary data.

   A whitened representation and (optional) natural gradient steps are used to aid
   model optimization.

   :param model: The GPflow :class:`~gpflow.models.VGP`.
   :param optimizer: The optimizer with which to train the model. Defaults to
       :class:`~trieste.models.optimizer.Optimizer` with :class:`~gpflow.optimizers.Scipy`.
   :param use_natgrads: If True then alternate model optimization steps with natural
       gradient updates. Note that natural gradients requires
       a :class:`~trieste.models.optimizer.BatchOptimizer` wrapper with
       :class:`~tf.optimizers.Optimizer` optimizer.
   :natgrad_gamma: Gamma parameter for the natural gradient optimizer.
   :raise ValueError (or InvalidArgumentError): If ``model``'s :attr:`q_sqrt` is not rank 3
       or if attempting to combine natural gradients with a :class:`~gpflow.optimizers.Scipy`
       optimizer.

   .. py:method:: model(self) -> gpflow.models.VGP
      :property:

      The underlying GPflow model.


   .. py:method:: update(self, dataset: trieste.data.Dataset, *, jitter: float = DEFAULTS.JITTER) -> None

      Update the model given the specified ``dataset``. Does not train the model.

      :param dataset: The data with which to update the model.
      :param jitter: The size of the jitter to use when stabilizing the Cholesky decomposition of
          the covariance matrix.


   .. py:method:: optimize(self, dataset: trieste.data.Dataset) -> None

      :class:`VariationalGaussianProcess` has a custom `optimize` method that (optionally) permits
      alternating between standard optimization steps (for kernel parameters) and natural gradient
      steps for the variational parameters (`q_mu` and `q_sqrt`). See :cite:`salimbeni2018natural`
      for details. Using natural gradients can dramatically speed up model fitting, especially for
      ill-conditioned posteriors.

      If using natural gradients, our optimizer inherits the mini-batch behavior and number
      of optimization steps as the base optimizer specified when initializing
      the :class:`VariationalGaussianProcess`.


   .. py:method:: get_internal_data(self) -> trieste.data.Dataset

      Return the model's training data.

      :return: The model's training data.


   .. py:method:: covariance_between_points(self, query_points_1: trieste.types.TensorType, query_points_2: trieste.types.TensorType) -> trieste.types.TensorType

      Compute the posterior covariance between sets of query points.

      Note that query_points_2 must be a rank 2 tensor, but query_points_1 can
      have leading dimensions.

      :param query_points_1: Set of query points with shape [..., A, D]
      :param query_points_2: Sets of query points with shape [B, D]
      :return: Covariance matrix between the sets of query points with shape [..., L, A, B]
          (L being the number of latent GPs = number of output dimensions)



.. py:class:: BatchReparametrizationSampler(sample_size: int, model: trieste.models.interfaces.SupportsPredictJoint)

   Bases: :py:obj:`trieste.models.interfaces.ReparametrizationSampler`\ [\ :py:obj:`trieste.models.interfaces.SupportsPredictJoint`\ ]

   This sampler employs the *reparameterization trick* to approximate batches of samples from a
   :class:`ProbabilisticModel`\ 's predictive joint distribution as

   .. math:: x \mapsto \mu(x) + \epsilon L(x)

   where :math:`L` is the Cholesky factor s.t. :math:`LL^T` is the covariance, and
   :math:`\epsilon \sim \mathcal N (0, 1)` is constant for a given sampler, thus ensuring samples
   form a continuous curve.

   :param sample_size: The number of samples for each batch of points. Must be positive.
   :param model: The model to sample from.
   :raise ValueError (or InvalidArgumentError): If ``sample_size`` is not positive.

   .. py:method:: sample(self, at: trieste.types.TensorType, *, jitter: float = DEFAULTS.JITTER) -> trieste.types.TensorType

      Return approximate samples from the `model` specified at :meth:`__init__`. Multiple calls to
      :meth:`sample`, for any given :class:`BatchReparametrizationSampler` and ``at``, will
      produce the exact same samples. Calls to :meth:`sample` on *different*
      :class:`BatchReparametrizationSampler` instances will produce different samples.

      :param at: Batches of query points at which to sample the predictive distribution, with
          shape `[..., B, D]`, for batches of size `B` of points of dimension `D`. Must have a
          consistent batch size across all calls to :meth:`sample` for any given
          :class:`BatchReparametrizationSampler`.
      :param jitter: The size of the jitter to use when stabilising the Cholesky decomposition of
          the covariance matrix.
      :return: The samples, of shape `[..., S, B, L]`, where `S` is the `sample_size`, `B` the
          number of points per batch, and `L` the dimension of the model's predictive
          distribution.
      :raise ValueError (or InvalidArgumentError): If any of the following are true:
          - ``at`` is a scalar.
          - The batch size `B` of ``at`` is not positive.
          - The batch size `B` of ``at`` differs from that of previous calls.
          - ``jitter`` is negative.



.. py:class:: IndependentReparametrizationSampler(sample_size: int, model: trieste.models.interfaces.ProbabilisticModel)

   Bases: :py:obj:`trieste.models.interfaces.ReparametrizationSampler`\ [\ :py:obj:`trieste.models.interfaces.ProbabilisticModel`\ ]

   This sampler employs the *reparameterization trick* to approximate samples from a
   :class:`ProbabilisticModel`\ 's predictive distribution as

   .. math:: x \mapsto \mu(x) + \epsilon \sigma(x)

   where :math:`\epsilon \sim \mathcal N (0, 1)` is constant for a given sampler, thus ensuring
   samples form a continuous curve.

   :param sample_size: The number of samples to take at each point. Must be positive.
   :param model: The model to sample from.
   :raise ValueError (or InvalidArgumentError): If ``sample_size`` is not positive.

   .. py:method:: sample(self, at: trieste.types.TensorType, *, jitter: float = DEFAULTS.JITTER) -> trieste.types.TensorType

      Return approximate samples from the `model` specified at :meth:`__init__`. Multiple calls to
      :meth:`sample`, for any given :class:`IndependentReparametrizationSampler` and ``at``, will
      produce the exact same samples. Calls to :meth:`sample` on *different*
      :class:`IndependentReparametrizationSampler` instances will produce different samples.

      :param at: Where to sample the predictive distribution, with shape `[..., 1, D]`, for points
          of dimension `D`.
      :param jitter: The size of the jitter to use when stabilising the Cholesky decomposition of
          the covariance matrix.
      :return: The samples, of shape `[..., S, 1, L]`, where `S` is the `sample_size` and `L` is
          the number of latent model dimensions.
      :raise ValueError (or InvalidArgumentError): If ``at`` has an invalid shape or ``jitter``
          is negative.



.. py:class:: RandomFourierFeatureTrajectorySampler(model: SupportsGetKernelObservationNoiseInternalData, num_features: int = 1000)

   Bases: :py:obj:`trieste.models.interfaces.TrajectorySampler`\ [\ :py:obj:`SupportsGetKernelObservationNoiseInternalData`\ ]

   This class builds functions that approximate a trajectory sampled from an underlying Gaussian
   process model. For tractibility, the Gaussian process is approximated with a Bayesian
   Linear model across a set of features sampled from the Fourier feature decomposition of
   the model's kernel. See :cite:`hernandez2014predictive` for details.

   Achieving consistency (ensuring that the same sample draw for all evalutions of a particular
   trajectory function) for exact sample draws from a GP is prohibitively costly because it scales
   cubically with the number of query points. However, finite feature representations can be
   evaluated with constant cost regardless of the required number of queries.

   In particular, we approximate the Gaussian processes' posterior samples as the finite feature
   approximation

   .. math:: \hat{f}(x) = \sum_{i=1}^m \phi_i(x)\theta_i

   where :math:`\phi_i` are m Fourier features and :math:`\theta_i` are
   feature weights sampled from a posterior distribution that depends on the feature values at the
   model's datapoints.

   Our implementation follows :cite:`hernandez2014predictive`, with our calculations
   differing slightly depending on properties of the problem. In particular,  we used different
   calculation strategies depending on the number of considered features m and the number
   of data points n.

   If :math:`m<n` then we follow Appendix A of :cite:`hernandez2014predictive` and calculate the
   posterior distribution for :math:`\theta` following their Bayesian linear regression motivation,
   i.e. the computation revolves around an O(m^3)  inversion of a design matrix.

   If :math:`n<m` then we use the kernel trick to recast computation to revolve around an O(n^3)
   inversion of a gram matrix. As well as being more efficient in early BO
   steps (where :math:`n<m`), this second computation method allows much larger choices
   of m (as required to approximate very flexible kernels).

   :param sample_size: The desired number of samples.
   :param model: The model to sample from.
   :param num_features: The number of features used to approximate the kernel. We use a default
       of 1000 as it typically perfoms well for a wide range of kernels. Note that very smooth
       kernels (e.g. RBF) can be well-approximated with fewer features.
   :raise ValueError: If ``dataset`` is empty.

   .. py:method:: _prepare_theta_posterior_in_design_space(self) -> tensorflow_probability.distributions.MultivariateNormalTriL

      Calculate the posterior of theta (the feature weights) in the design space. This
      distribution is a Gaussian

      .. math:: \theta \sim N(D^{-1}\Phi^Ty,D^{-1}\sigma^2)

      where the [m,m] design matrix :math:`D=(\Phi^T\Phi + \sigma^2I_m)` is defined for
      the [n,m] matrix of feature evaluations across the training data :math:`\Phi`
      and observation noise variance :math:`\sigma^2`.


   .. py:method:: _prepare_theta_posterior_in_gram_space(self) -> tensorflow_probability.distributions.MultivariateNormalTriL

      Calculate the posterior of theta (the feature weights) in the gram space.

       .. math:: \theta \sim N(\Phi^TG^{-1}y,I_m - \Phi^TG^{-1}\Phi)

      where the [n,n] gram matrix :math:`G=(\Phi\Phi^T + \sigma^2I_n)` is defined for the [n,m]
      matrix of feature evaluations across the training data :math:`\Phi` and
      observation noise variance :math:`\sigma^2`.


   .. py:method:: get_trajectory(self) -> trieste.models.interfaces.TrajectoryFunction

      Generate an approximate function draw (trajectory) by sampling weights
      and evaluating the feature functions.

      :return: A trajectory function representing an approximate trajectory from the Gaussian
          process, taking an input of shape `[N, D]` and returning shape `[N, 1]`


   .. py:method:: update_trajectory(self, trajectory: trieste.models.interfaces.TrajectoryFunction) -> trieste.models.interfaces.TrajectoryFunction

      Efficiently update a :const:`TrajectoryFunction` to reflect an update in its
      underlying :class:`ProbabilisticModel` and resample accordingly.

      For a :class:`RandomFourierFeatureTrajectorySampler`, updating the sampler
      corresponds to resampling the feature functions (taking into account any
      changed kernel parameters) and recalculating the weight distribution.

      :param trajectory: The trajectory function to be resampled.
      :return: The new resampled trajectory function.


   .. py:method:: resample_trajectory(self, trajectory: trieste.models.interfaces.TrajectoryFunction) -> trieste.models.interfaces.TrajectoryFunction

      Efficiently resample a :const:`TrajectoryFunction` in-place to avoid function retracing
      with every new sample.

      :param trajectory: The trajectory function to be resampled.
      :return: The new resampled trajectory function.



.. py:class:: fourier_feature_trajectory(feature_functions: gpflux.layers.basis_functions.fourier_features.RandomFourierFeaturesCosine, weight_distribution: tensorflow_probability.distributions.MultivariateNormalTriL)

   Bases: :py:obj:`trieste.models.interfaces.TrajectoryFunctionClass`

   An approximate sample from a Gaussian processes' posterior samples represented as a
   finite weighted sum of features.

   A trajectory is given by

   .. math:: \hat{f}(x) = \sum_{i=1}^m \phi_i(x)\theta_i

   where :math:`\phi_i` are m feature functions and :math:`\theta_i` are
   feature weights sampled from a posterior distribution.

   The number of trajectories (i.e. batch size) is determined from the first call of the
   trajectory. In order to change the batch size, a new :class:`TrajectoryFunction` must be built.

   :param feature_functions: Set of feature function.
   :param weight_distribution: Distribution from which feature weights are to be sampled.

   .. py:method:: __call__(self, x: trieste.types.TensorType) -> trieste.types.TensorType

      Call trajectory function.


   .. py:method:: resample(self) -> None

      Efficiently resample in-place without retracing.


   .. py:method:: update(self, weight_distribution: tensorflow_probability.distributions.MultivariateNormalTriL) -> None

      Efficiently update the trajectory with a new weight distribution and resample its weights.

      :param weight_distribution: new distribution from which feature weights are to be sampled.



.. py:function:: check_optimizer(optimizer: Union[trieste.models.optimizer.BatchOptimizer, trieste.models.optimizer.Optimizer]) -> None

   Check that the optimizer for the GPflow models is using a correct optimizer wrapper.

   Stochastic gradient descent based methods implemented in TensorFlow would not
   work properly without mini-batches and hence :class:`~trieste.models.optimizers.BatchOptimizer`
   that prepares mini-batches and calls the optimizer iteratively needs to be used. GPflow's
   :class:`~gpflow.optimizers.Scipy` optimizer on the other hand should use the non-batch wrapper
   :class:`~trieste.models.optimizers.Optimizer`.

   :param optimizer: An instance of the optimizer wrapper with the underlying optimizer.
   :raise ValueError: If :class:`~tf.optimizers.Optimizer` is not using
       :class:`~trieste.models.optimizers.BatchOptimizer` or :class:`~gpflow.optimizers.Scipy` is
       using :class:`~trieste.models.optimizers.BatchOptimizer`.


.. py:function:: randomize_hyperparameters(object: gpflow.Module) -> None

   Sets hyperparameters to random samples from their constrained domains or (if not constraints
   are available) their prior distributions.

   :param object: Any gpflow Module.


.. py:function:: squeeze_hyperparameters(object: gpflow.Module, alpha: float = 0.01, epsilon: float = 1e-07) -> None

   Squeezes the parameters to be strictly inside their range defined by the Sigmoid,
   or strictly greater than the limit defined by the Shift+Softplus.
   This avoids having Inf unconstrained values when the parameters are exactly at the boundary.

   :param object: Any gpflow Module.
   :param alpha: the proportion of the range with which to squeeze for the Sigmoid case
   :param epsilon: the value with which to offset the shift for the Softplus case.
   :raise ValueError: If ``alpha`` is not in (0,1) or epsilon <= 0


