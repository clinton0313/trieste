:orphan:

:py:mod:`trieste.models.gpflow.models`
======================================

.. py:module:: trieste.models.gpflow.models


Module Contents
---------------

.. py:class:: GaussianProcessRegression(model: GPR | SGPR, optimizer: Optimizer | None = None, num_kernel_samples: int = 10, num_rff_features: int = 1000)

   Bases: :py:obj:`trieste.models.gpflow.interface.GPflowPredictor`, :py:obj:`trieste.models.interfaces.TrainableProbabilisticModel`, :py:obj:`trieste.models.interfaces.FastUpdateModel`, :py:obj:`trieste.models.gpflow.interface.SupportsCovarianceBetweenPoints`, :py:obj:`trieste.models.interfaces.SupportsInternalData`, :py:obj:`trieste.models.interfaces.HasTrajectorySampler`

   A :class:`TrainableProbabilisticModel` wrapper for a GPflow :class:`~gpflow.models.GPR`
   or :class:`~gpflow.models.SGPR`.

   :param model: The GPflow model to wrap.
   :param optimizer: The optimizer with which to train the model. Defaults to
       :class:`~trieste.models.optimizer.Optimizer` with :class:`~gpflow.optimizers.Scipy`.
   :param num_kernel_samples: Number of randomly sampled kernels (for each kernel parameter) to
       evaluate before beginning model optimization. Therefore, for a kernel with `p`
       (vector-valued) parameters, we evaluate `p * num_kernel_samples` kernels.
   :param num_rff_features: The number of random Foruier features used to approximate the
       kernel when calling :meth:`trajectory_sampler`. We use a default of 1000 as it
       typically perfoms well for a wide range of kernels. Note that very smooth
       kernels (e.g. RBF) can be well-approximated with fewer features.

   .. py:method:: model(self) -> GPR | SGPR
      :property:

      The underlying GPflow model.


   .. py:method:: update(self, dataset: trieste.data.Dataset) -> None

      Update the model given the specified ``dataset``. Does not train the model.

      :param dataset: The data with which to update the model.


   .. py:method:: covariance_between_points(self, query_points_1: trieste.types.TensorType, query_points_2: trieste.types.TensorType) -> trieste.types.TensorType

      Compute the posterior covariance between sets of query points.

      .. math:: \Sigma_{12} = K_{12} - K_{x1}(K_{xx} + \sigma^2 I)^{-1}K_{x2}

      Note that query_points_2 must be a rank 2 tensor, but query_points_1 can
      have leading dimensions.

      :param query_points_1: Set of query points with shape [..., N, D]
      :param query_points_2: Sets of query points with shape [M, D]
      :return: Covariance matrix between the sets of query points with shape [..., L, N, M]
          (L being the number of latent GPs = number of output dimensions)


   .. py:method:: optimize(self, dataset: trieste.data.Dataset) -> None

      Optimize the model with the specified `dataset`.

      For :class:`GaussianProcessRegression`, we (optionally) try multiple randomly sampled
      kernel parameter configurations as well as the configuration specified when initializing
      the kernel. The best configuration is used as the starting point for model optimization.

      For trainable parameters constrained to lie in a finite interval (through a sigmoid
      bijector), we begin model optimization from the best of a random sample from these
      parameters' acceptable domains.

      For trainable parameters without constraints but with priors, we begin model optimization
      from the best of a random sample from these parameters' priors.

      For trainable parameters with neither priors nor constraints, we begin optimization from
      their initial values.

      :param dataset: The data with which to optimize the `model`.


   .. py:method:: find_best_model_initialization(self, num_kernel_samples: int) -> None

      Test `num_kernel_samples` models with sampled kernel parameters. The model's kernel
      parameters are then set to the sample achieving maximal likelihood.

      :param num_kernel_samples: Number of randomly sampled kernels to evaluate.


   .. py:method:: trajectory_sampler(self) -> trieste.models.interfaces.TrajectorySampler[GaussianProcessRegression]

      Return a trajectory sampler. For :class:`GaussianProcessRegression`, we build
      trajectories using a random Fourier feature approximation.

      :return: The trajectory sampler.


   .. py:method:: get_internal_data(self) -> trieste.data.Dataset

      Return the model's training data.

      :return: The model's training data.


   .. py:method:: conditional_predict_f(self, query_points: trieste.types.TensorType, additional_data: trieste.data.Dataset) -> tuple[trieste.types.TensorType, trieste.types.TensorType]

      Returns the marginal GP distribution at query_points conditioned on both the model
      and some additional data, using exact formula. See :cite:`chevalier2014corrected`
      (eqs. 8-10) for details.

      :param query_points: Set of query points with shape [M, D]
      :param additional_data: Dataset with query_points with shape [..., N, D] and observations
               with shape [..., N, L]
      :return: mean_qp_new: predictive mean at query_points, with shape [..., M, L],
               and var_qp_new: predictive variance at query_points, with shape [..., M, L]


   .. py:method:: conditional_predict_joint(self, query_points: trieste.types.TensorType, additional_data: trieste.data.Dataset) -> tuple[trieste.types.TensorType, trieste.types.TensorType]

      Predicts the joint GP distribution at query_points conditioned on both the model
      and some additional data, using exact formula. See :cite:`chevalier2014corrected`
      (eqs. 8-10) for details.

      :param query_points: Set of query points with shape [M, D]
      :param additional_data: Dataset with query_points with shape [..., N, D] and observations
               with shape [..., N, L]
      :return: mean_qp_new: predictive mean at query_points, with shape [..., M, L],
               and cov_qp_new: predictive covariance between query_points, with shape
               [..., L, M, M]


   .. py:method:: conditional_predict_f_sample(self, query_points: trieste.types.TensorType, additional_data: trieste.data.Dataset, num_samples: int) -> trieste.types.TensorType

      Generates samples of the GP at query_points conditioned on both the model
      and some additional data.

      :param query_points: Set of query points with shape [M, D]
      :param additional_data: Dataset with query_points with shape [..., N, D] and observations
               with shape [..., N, L]
      :param num_samples: number of samples
      :return: samples of f at query points, with shape [..., num_samples, M, L]


   .. py:method:: conditional_predict_y(self, query_points: trieste.types.TensorType, additional_data: trieste.data.Dataset) -> tuple[trieste.types.TensorType, trieste.types.TensorType]

      Generates samples of y from the GP at query_points conditioned on both the model
      and some additional data.

      :param query_points: Set of query points with shape [M, D]
      :param additional_data: Dataset with query_points with shape [..., N, D] and observations
               with shape [..., N, L]
      :return: predictive variance at query_points, with shape [..., M, L],
               and predictive variance at query_points, with shape [..., M, L]



.. py:class:: NumDataPropertyMixin

   Mixin class for exposing num_data as a property, stored in a tf.Variable. This is to work
   around GPFlow storing it as a number, which prevents us from updating it without retracing.
   The property is required due to the way num_data is used in the model elbo methods.

   Note that this doesn't support a num_data value of None.

   This should be removed once GPFlow is updated to support Variables as num_data.


.. py:class:: Parameter(value: gpflow.base.TensorData, *, transform: Optional[gpflow.base.Transform] = None, prior: Optional[gpflow.base.Prior] = None, prior_on: Union[str, gpflow.base.PriorOn] = PriorOn.CONSTRAINED, trainable: bool = True, dtype: Optional[gpflow.base.DType] = None, name: Optional[str] = None, transformed_shape: Any = None, pretransformed_shape: Any = None)

   Bases: :py:obj:`gpflow.Parameter`

   A modified version of gpflow.Parameter that supports variable shapes.

   A copy of gpflow.Parameter's init but with additional shape arguments that are passed
   to a modified TransformedVariable.


.. py:class:: SparseVariational(model: gpflow.models.SVGP, optimizer: Optimizer | None = None)

   Bases: :py:obj:`trieste.models.gpflow.interface.GPflowPredictor`, :py:obj:`trieste.models.interfaces.TrainableProbabilisticModel`, :py:obj:`trieste.models.gpflow.interface.SupportsCovarianceBetweenPoints`

   A :class:`TrainableProbabilisticModel` wrapper for a GPflow :class:`~gpflow.models.SVGP`.

   :param model: The underlying GPflow sparse variational model.
   :param optimizer: The optimizer with which to train the model. Defaults to
       :class:`~trieste.models.optimizer.BatchOptimizer` with :class:`~tf.optimizers.Adam` with
       batch size 100.

   .. py:method:: model(self) -> gpflow.models.SVGP
      :property:

      The underlying GPflow model.


   .. py:method:: update(self, dataset: trieste.data.Dataset) -> None

      Update the model given the specified ``dataset``. Does not train the model.

      :param dataset: The data with which to update the model.


   .. py:method:: covariance_between_points(self, query_points_1: trieste.types.TensorType, query_points_2: trieste.types.TensorType) -> trieste.types.TensorType

      Compute the posterior covariance between sets of query points.

      Note that query_points_2 must be a rank 2 tensor, but query_points_1 can
      have leading dimensions.

      :param query_points_1: Set of query points with shape [..., A, D]
      :param query_points_2: Sets of query points with shape [B, D]
      :return: Covariance matrix between the sets of query points with shape [..., L, A, B]
          (L being the number of latent GPs = number of output dimensions)



.. py:class:: VariationalGaussianProcess(model: gpflow.models.VGP, optimizer: Optimizer | None = None, use_natgrads: bool = False, natgrad_gamma: Optional[float] = None)

   Bases: :py:obj:`trieste.models.gpflow.interface.GPflowPredictor`, :py:obj:`trieste.models.interfaces.TrainableProbabilisticModel`, :py:obj:`trieste.models.interfaces.SupportsInternalData`, :py:obj:`trieste.models.gpflow.interface.SupportsCovarianceBetweenPoints`

   A :class:`TrainableProbabilisticModel` wrapper for a GPflow :class:`~gpflow.models.VGP`.

   A Variational Gaussian Process (VGP) approximates the posterior of a GP
   using the multivariate Gaussian closest to the posterior of the GP by minimizing the
   KL divergence between approximated and exact posteriors. See :cite:`opper2009variational`
   for details.

   The VGP provides (approximate) GP modelling under non-Gaussian likelihoods, for example
   when fitting a classification model over binary data.

   A whitened representation and (optional) natural gradient steps are used to aid
   model optimization.

   :param model: The GPflow :class:`~gpflow.models.VGP`.
   :param optimizer: The optimizer with which to train the model. Defaults to
       :class:`~trieste.models.optimizer.Optimizer` with :class:`~gpflow.optimizers.Scipy`.
   :param use_natgrads: If True then alternate model optimization steps with natural
       gradient updates. Note that natural gradients requires
       a :class:`~trieste.models.optimizer.BatchOptimizer` wrapper with
       :class:`~tf.optimizers.Optimizer` optimizer.
   :natgrad_gamma: Gamma parameter for the natural gradient optimizer.
   :raise ValueError (or InvalidArgumentError): If ``model``'s :attr:`q_sqrt` is not rank 3
       or if attempting to combine natural gradients with a :class:`~gpflow.optimizers.Scipy`
       optimizer.

   .. py:method:: model(self) -> gpflow.models.VGP
      :property:

      The underlying GPflow model.


   .. py:method:: update(self, dataset: trieste.data.Dataset, *, jitter: float = DEFAULTS.JITTER) -> None

      Update the model given the specified ``dataset``. Does not train the model.

      :param dataset: The data with which to update the model.
      :param jitter: The size of the jitter to use when stabilizing the Cholesky decomposition of
          the covariance matrix.


   .. py:method:: optimize(self, dataset: trieste.data.Dataset) -> None

      :class:`VariationalGaussianProcess` has a custom `optimize` method that (optionally) permits
      alternating between standard optimization steps (for kernel parameters) and natural gradient
      steps for the variational parameters (`q_mu` and `q_sqrt`). See :cite:`salimbeni2018natural`
      for details. Using natural gradients can dramatically speed up model fitting, especially for
      ill-conditioned posteriors.

      If using natural gradients, our optimizer inherits the mini-batch behavior and number
      of optimization steps as the base optimizer specified when initializing
      the :class:`VariationalGaussianProcess`.


   .. py:method:: get_internal_data(self) -> trieste.data.Dataset

      Return the model's training data.

      :return: The model's training data.


   .. py:method:: covariance_between_points(self, query_points_1: trieste.types.TensorType, query_points_2: trieste.types.TensorType) -> trieste.types.TensorType

      Compute the posterior covariance between sets of query points.

      Note that query_points_2 must be a rank 2 tensor, but query_points_1 can
      have leading dimensions.

      :param query_points_1: Set of query points with shape [..., A, D]
      :param query_points_2: Sets of query points with shape [B, D]
      :return: Covariance matrix between the sets of query points with shape [..., L, A, B]
          (L being the number of latent GPs = number of output dimensions)



.. py:function:: _covariance_between_points_for_variational_models(kernel: gpflow.kernels.Kernel, inducing_variable: gpflow.inducing_variables.InducingVariables, q_sqrt: trieste.types.TensorType, query_points_1: trieste.types.TensorType, query_points_2: trieste.types.TensorType, whiten: bool) -> trieste.types.TensorType

   Compute the posterior covariance between sets of query points.

   .. math:: \Sigma_{12} = K_{1x}BK_{x2} + K_{12} - K_{1x}K_{xx}^{-1}K_{x2}

   where :math:`B = K_{xx}^{-1}(q_{sqrt}q_{sqrt}^T)K_{xx}^{-1}`
   or :math:`B = L^{-1}(q_{sqrt}q_{sqrt}^T)(L^{-1})^T` if we are using
   a whitened representation in our variational approximation. Here
   :math:`L` is the Cholesky decomposition of :math:`K_{xx}`.
   See :cite:`titsias2009variational` for a derivation.

   Note that this function can also be applied to
   our :class:`VariationalGaussianProcess` models by passing in the training
   data rather than the locations of the inducing points.

   Although query_points_2 must be a rank 2 tensor, query_points_1 can
   have leading dimensions.

   :inducing points: The input locations chosen for our variational approximation.
   :q_sqrt: The Cholesky decomposition of the covariance matrix of our
       variational distribution.
   :param query_points_1: Set of query points with shape [..., A, D]
   :param query_points_2: Sets of query points with shape [B, D]
   :param whiten:  If True then use whitened representations.
   :return: Covariance matrix between the sets of query points with shape [..., L, A, B]
       (L being the number of latent GPs = number of output dimensions)


.. py:function:: _compute_kernel_blocks(kernel: gpflow.kernels.Kernel, inducing_variable: gpflow.inducing_variables.InducingVariables, query_points_1: trieste.types.TensorType, query_points_2: trieste.types.TensorType, num_latent: int) -> tuple[trieste.types.TensorType, trieste.types.TensorType, trieste.types.TensorType, trieste.types.TensorType]

   Return all the prior covariances required to calculate posterior covariances for each latent
   Gaussian process, as specified by the `num_latent` input.

   This function returns the covariance between: `inducing_variables` and `query_points_1`;
   `inducing_variables` and `query_points_2`; `query_points_1` and `query_points_2`;
   `inducing_variables` and `inducing_variables`.

   The calculations are performed differently depending on the type of
   kernel (single output, separate independent multi-output or shared independent
   multi-output) and inducing variables (simple set, SharedIndependent or SeparateIndependent).

   Note that `num_latents` is only used when we use a single kernel for a multi-output model.


