:py:mod:`trieste.models.gpflux`
===============================

.. py:module:: trieste.models.gpflux

.. autoapi-nested-parse::

   This package contains the primary interface for deep Gaussian process models. It also contains a
   number of :class:`TrainableProbabilisticModel` wrappers for GPflux-based models. Note that currently
   copying/saving models is not supported, so in a Bayes Opt loop `track_state` should be set False.
   Note as well that `tf.keras.backend.set_floatx()` should be used to set the desired float type,
   consistent with the GPflow float type being used.



Submodules
----------
.. toctree::
   :titlesonly:
   :maxdepth: 1

   architectures/index.rst


Package Contents
----------------

.. py:function:: build_vanilla_deep_gp(query_points: trieste.types.TensorType, num_layers: int, num_inducing: int, inner_layer_sqrt_factor: float = 1e-05, likelihood_noise_variance: float = 0.01, search_space: Optional[trieste.space.Box] = None) -> gpflux.models.DeepGP

   Provides a wrapper around `build_constant_input_dim_deep_gp` from `gpflux.architectures`.

   :param query_points: input data, used to determine inducing point locations with k-means.
   :param num_layers: number of layers in deep GP.
   :param num_inducing: number of inducing points to use in each layer.
   :param inner_layer_sqrt_factor: A multiplicative factor used to rescale hidden layers
   :param likelihood_noise_variance: initial noise variance
   :param search_space: the search space for the Bayes Opt problem. Used for initialization of
       inducing locations if num_inducing > len(query_points)
   :return: :class:`gpflux.models.DeepGP`


.. py:class:: GPfluxPredictor(optimizer: BatchOptimizer | None = None)

   Bases: :py:obj:`trieste.models.interfaces.SupportsGetObservationNoise`, :py:obj:`abc.ABC`

   A trainable wrapper for a GPflux deep Gaussian process model. The code assumes subclasses
   will use the Keras `fit` method for training, and so they should provide access to both a
   `model_keras` and `model_gpflux`. Note: due to Keras integration, the user should remember to
   use `tf.keras.backend.set_floatx()` with the desired value (consistent with GPflow) to avoid
   dtype errors.

   :param optimizer: The optimizer with which to train the model. Defaults to
       :class:`~trieste.models.optimizer.BatchOptimizer` with :class:`~tf.optimizers.Adam`.

   .. py:method:: model_gpflux(self) -> gpflow.base.Module
      :property:

      The underlying GPflux model.


   .. py:method:: model_keras(self) -> tensorflow.keras.Model
      :property:

      Returns the compiled Keras model for training.


   .. py:method:: optimizer(self) -> trieste.models.optimizer.BatchOptimizer
      :property:

      The optimizer with which to train the model.


   .. py:method:: predict(self, query_points: trieste.types.TensorType) -> tuple[trieste.types.TensorType, trieste.types.TensorType]

      Note: unless otherwise noted, this returns the mean and variance of the last layer
      conditioned on one sample from the previous layers.


   .. py:method:: sample(self, query_points: trieste.types.TensorType, num_samples: int) -> trieste.types.TensorType
      :abstractmethod:

      Return ``num_samples`` samples from the independent marginal distributions at
      ``query_points``.

      :param query_points: The points at which to sample, with shape [..., N, D].
      :param num_samples: The number of samples at each point.
      :return: The samples. For a predictive distribution with event shape E, this has shape
          [..., S, N] + E, where S is the number of samples.


   .. py:method:: predict_y(self, query_points: trieste.types.TensorType) -> tuple[trieste.types.TensorType, trieste.types.TensorType]

      Note: unless otherwise noted, this will return the prediction conditioned on one sample
      from the lower layers.


   .. py:method:: get_observation_noise(self) -> trieste.types.TensorType

      Return the variance of observation noise for homoscedastic likelihoods.

      :return: The observation noise.
      :raise NotImplementedError: If the model does not have a homoscedastic likelihood.



.. py:class:: DeepGaussianProcess(model: gpflux.models.DeepGP, optimizer: BatchOptimizer | None = None, continuous_optimisation: bool = True)

   Bases: :py:obj:`trieste.models.gpflux.interface.GPfluxPredictor`, :py:obj:`trieste.models.interfaces.TrainableProbabilisticModel`

   A :class:`TrainableProbabilisticModel` wrapper for a GPflux :class:`~gpflux.models.DeepGP` with
   :class:`GPLayer` or :class:`LatentVariableLayer`: this class does not support e.g. keras layers.
   We provide simple architectures that can be used with this class in the `architectures.py` file.
   Note: the user should remember to set `tf.keras.backend.set_floatx()` with the desired value
   (consistent with GPflow) so that dtype errors do not occur.

   :param model: The underlying GPflux deep Gaussian process model.
   :param optimizer: The optimizer configuration for training the model. Defaults to
       :class:`~trieste.models.optimizer.BatchOptimizer` wrapper with
       :class:`~tf.optimizers.Adam`.
       This wrapper itself is not used, instead only its `optimizer` and `minimize_args` are
       used. Its optimizer is used when compiling a Keras GPflux model and `minimize_args` is
       a dictionary of arguments to be used in the Keras `fit` method. Defaults to
       using 100 epochs, batch size 100, and verbose 0. See
       https://keras.io/api/models/model_training_apis/#fit-method for a list of possible
       arguments.
   :param continuous_optimisation: if True (default), the optimizer will keep track of the
       number of epochs across BO iterations and use this number as initial_epoch. This is
       essential to allow monitoring of model training across BO iterations.

   .. py:method:: model_gpflux(self) -> gpflux.models.DeepGP
      :property:

      The underlying GPflux model.


   .. py:method:: model_keras(self) -> tensorflow.keras.Model
      :property:

      Returns the compiled Keras model for training.


   .. py:method:: sample(self, query_points: trieste.types.TensorType, num_samples: int) -> trieste.types.TensorType

      Return ``num_samples`` samples from the independent marginal distributions at
      ``query_points``.

      :param query_points: The points at which to sample, with shape [..., N, D].
      :param num_samples: The number of samples at each point.
      :return: The samples. For a predictive distribution with event shape E, this has shape
          [..., S, N] + E, where S is the number of samples.


   .. py:method:: update(self, dataset: trieste.data.Dataset) -> None

      Update the model given the specified ``dataset``. Does not train the model.

      :param dataset: The data with which to update the model.


   .. py:method:: optimize(self, dataset: trieste.data.Dataset) -> None

      Optimize the model with the specified `dataset`.
      :param dataset: The data with which to optimize the `model`.



