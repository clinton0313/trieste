{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-07 21:11:22.296838: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-07 21:11:22.296880: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.get_logger().setLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-07 21:11:24.856407: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-04-07 21:11:24.856629: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-04-07 21:11:24.856642: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-04-07 21:11:24.856666: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (clinton-ThinkPad-T450): /proc/driver/nvidia/version does not exist\n",
      "2022-04-07 21:11:24.856927: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-07 21:11:24.857639: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    }
   ],
   "source": [
    "y_pred = tf.constant([[1, 0.5, 0.4, 0.4], [2, 0.3, 0.1, 0.4]])\n",
    "y_true = tf.constant([[1.], [1.8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_evidential_output(y_true, y_pred):\n",
    "    if y_true.shape.rank == 1:\n",
    "        y_true = tf.expand_dims(y_true, axis=-1)\n",
    "    if y_pred.shape.rank == 1:\n",
    "        y_pred = tf.expand_dims(y_pred, axis=0)\n",
    "    assert y_pred.shape[1] == 4, f\"Expects four parameters for the evidential output instead got {y_pred.shape[1]}\"\n",
    "\n",
    "    gamma, lamb, alpha, beta = tf.split(y_pred, 4, axis=-1)\n",
    "    \n",
    "    return y_true, gamma, lamb, alpha, beta\n",
    "\n",
    "def normal_inverse_gamma_negative_log_likelihood(y_true, gamma, lamb, alpha, beta):\n",
    "\n",
    "    negative_log_likelihood = -(\n",
    "            tf.math.log(2**(0.5 + alpha))\n",
    "            + tf.math.log(beta**alpha)\n",
    "            + 0.5 *tf.math.log(lamb/ (2 * np.math.pi * ( 1 + lamb)))\n",
    "            - (0.5 + alpha) * tf.math.log(2 * beta + lamb * (gamma - y_true)**2 / (1 + lamb))\n",
    "            # + tf.math.lgamma(alpha)\n",
    "            # - tf.math.lgamma(alpha+0.5)\n",
    "        )\n",
    "    return negative_log_likelihood\n",
    "\n",
    "def normal_inverse_gamma_sum_of_squares(y_true, gamma, lamb, alpha, beta):\n",
    "\n",
    "    log_likelihood = (\n",
    "        tf.math.log(beta*(1 + lamb)/lamb + (alpha - 1)*(y_true - gamma)**2)\n",
    "        + tf.math.lgamma(alpha - 1)\n",
    "        - tf.math.lgamma(alpha)\n",
    "    )\n",
    "\n",
    "    return log_likelihood\n",
    "\n",
    "def normal_inverse_gamma_regularizer(y_true, gamma, lamb, alpha):\n",
    "    return tf.abs(y_true - gamma) * (2*alpha + lamb)\n",
    "\n",
    "def evidential_regression_loss(y_true, y_pred, coeff=1.0, loss_fn=normal_inverse_gamma_sum_of_squares):\n",
    "    if y_true.shape.rank == 1:\n",
    "        y_true = tf.expand_dims(y_true, axis=-1)\n",
    "    if y_pred.shape.rank == 1:\n",
    "        y_pred = tf.expand_dims(y_pred, axis=0)\n",
    "    assert y_pred.shape[1] == 4, f\"Expects four parameters for the evidential output instead got {y_pred.shape[1]}\"\n",
    "\n",
    "    gamma, lamb, alpha, beta = tf.split(y_pred, 4, axis=-1)\n",
    "\n",
    "    loss = tf.reduce_mean(loss_fn(y_true, gamma, lamb, alpha, beta))\n",
    "\n",
    "    regularizer = tf.reduce_mean(normal_inverse_gamma_regularizer(y_true, gamma, lamb, alpha))\n",
    "\n",
    "    return loss + coeff * regularizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tf.constant([[1, 0.5, 0.4, 0.3], [2, 0.3, 0.1, 0.7]])\n",
    "y_true = tf.constant([[1.], [1.8]])\n",
    "nll_reg_mean = 1.1719854\n",
    "nll_reg_mean_gamma = 2.4643755\n",
    "nll_sos_mean = 0.85427415"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.1719854>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def NIG_NLL(y, y_pred):\n",
    "    y, gamma, v, alpha, beta = prepare_evidential_output(y_true, y_pred)\n",
    "    twoBlambda = 2*beta*(1+v)\n",
    "\n",
    "    nll = 0.5*tf.math.log(np.pi/v)  \\\n",
    "        - alpha*tf.math.log(twoBlambda)  \\\n",
    "        + (alpha+0.5) * tf.math.log(v*(y-gamma)**2 + twoBlambda)  \\\n",
    "        # + tf.math.lgamma(alpha)  \\\n",
    "        # - tf.math.lgamma(alpha+0.5)\n",
    "    return nll\n",
    "\n",
    "tf.reduce_mean(NIG_NLL(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       "array([[0.8662584],\n",
       "       [1.4777125]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true, gamma, v, alpha, beta = prepare_evidential_output(y_true, y_pred)\n",
    "normal_inverse_gamma_negative_log_likelihood(y_true, gamma, v, alpha, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       "array([[0.4054652],\n",
       "       [1.203083 ]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_inverse_gamma_sum_of_squares(y_true, gamma, v, alpha, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def deep_evidential_regression_loss(\n",
    "    y_true, \n",
    "    y_pred, \n",
    "    coeff: float = 1, \n",
    "    loss_fn = normal_inverse_gamma_sum_of_squares\n",
    "):\n",
    "    '''\n",
    "    Maximum likelihood objective for deep evidential regression model using negative \n",
    "    log likelihood or sum of squares loss of the normal inverse gamma distribution.\n",
    "\n",
    "    :param y_true: The output variable values.\n",
    "    :param y_pred: The four output parameters of the deep evidential regression model\n",
    "        that characterize the normal inverse gamma distribution given in the order: gamma,\n",
    "        lambda, alpha, beta.\n",
    "    :param coeff: Regularization weight coefficient.\n",
    "    :param loss_fn: The base loss function used. By default we use the recommended loss function\n",
    "        based on the sum of squares. An alternate loss function is also defined based on the negative\n",
    "        log likelihood :function: `~trieste.models.keras.utils.normal_inverse_gamma_negative_log_likelihood`.\n",
    "    :return: The model evidence values.  \n",
    "    '''\n",
    "    if y_true.shape.rank == 1:\n",
    "        y_true = tf.expand_dims(y_true, axis=-1)\n",
    "    if y_pred.shape.rank == 1:\n",
    "        y_pred = tf.expand_dims(y_pred, axis=0)\n",
    "    tf.debugging.assert_shapes([(y_pred, (y_pred.shape[0], 4))])\n",
    "\n",
    "    gamma, lamb, alpha, beta = tf.split(y_pred, 4, axis=-1)\n",
    "\n",
    "    loss = loss_fn(y_true, gamma, lamb, alpha, beta)\n",
    "    regularization = normal_inverse_gamma_regularizer(y_true, gamma, lamb, alpha)\n",
    "\n",
    "    return tf.reduce_mean(loss + coeff * regularization, axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'rank'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/clinton/Documents/bse/masters_thesis/trieste/tests/scripts/loss_script.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/clinton/Documents/bse/masters_thesis/trieste/tests/scripts/loss_script.ipynb#ch0000011?line=1'>2</a>\u001b[0m y_pred \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconstant([\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/clinton/Documents/bse/masters_thesis/trieste/tests/scripts/loss_script.ipynb#ch0000011?line=2'>3</a>\u001b[0m                 [\u001b[39m1.\u001b[39m, \u001b[39m0.5\u001b[39m, \u001b[39m0.4\u001b[39m, \u001b[39m0.3\u001b[39m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/clinton/Documents/bse/masters_thesis/trieste/tests/scripts/loss_script.ipynb#ch0000011?line=3'>4</a>\u001b[0m                 [\u001b[39m2.\u001b[39m, \u001b[39m0.3\u001b[39m, \u001b[39m0.1\u001b[39m, \u001b[39m0.7\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/clinton/Documents/bse/masters_thesis/trieste/tests/scripts/loss_script.ipynb#ch0000011?line=4'>5</a>\u001b[0m             ])\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/clinton/Documents/bse/masters_thesis/trieste/tests/scripts/loss_script.ipynb#ch0000011?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtesting\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnpt\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/clinton/Documents/bse/masters_thesis/trieste/tests/scripts/loss_script.ipynb#ch0000011?line=7'>8</a>\u001b[0m npt\u001b[39m.\u001b[39massert_almost_equal(deep_evidential_regression_loss(y_true, y_pred), \u001b[39m0.85427415\u001b[39m)\n",
      "\u001b[1;32m/home/clinton/Documents/bse/masters_thesis/trieste/tests/scripts/loss_script.ipynb Cell 8'\u001b[0m in \u001b[0;36mdeep_evidential_regression_loss\u001b[0;34m(y_true, y_pred, coeff, loss_fn)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/clinton/Documents/bse/masters_thesis/trieste/tests/scripts/loss_script.ipynb#ch0000013?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeep_evidential_regression_loss\u001b[39m(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/clinton/Documents/bse/masters_thesis/trieste/tests/scripts/loss_script.ipynb#ch0000013?line=1'>2</a>\u001b[0m     y_true, \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/clinton/Documents/bse/masters_thesis/trieste/tests/scripts/loss_script.ipynb#ch0000013?line=2'>3</a>\u001b[0m     y_pred, \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/clinton/Documents/bse/masters_thesis/trieste/tests/scripts/loss_script.ipynb#ch0000013?line=3'>4</a>\u001b[0m     coeff: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/clinton/Documents/bse/masters_thesis/trieste/tests/scripts/loss_script.ipynb#ch0000013?line=4'>5</a>\u001b[0m     loss_fn \u001b[39m=\u001b[39m normal_inverse_gamma_sum_of_squares\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/clinton/Documents/bse/masters_thesis/trieste/tests/scripts/loss_script.ipynb#ch0000013?line=5'>6</a>\u001b[0m ):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/clinton/Documents/bse/masters_thesis/trieste/tests/scripts/loss_script.ipynb#ch0000013?line=6'>7</a>\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/clinton/Documents/bse/masters_thesis/trieste/tests/scripts/loss_script.ipynb#ch0000013?line=7'>8</a>\u001b[0m \u001b[39m    Maximum likelihood objective for deep evidential regression model using negative \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/clinton/Documents/bse/masters_thesis/trieste/tests/scripts/loss_script.ipynb#ch0000013?line=8'>9</a>\u001b[0m \u001b[39m    log likelihood or sum of squares loss of the normal inverse gamma distribution.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/clinton/Documents/bse/masters_thesis/trieste/tests/scripts/loss_script.ipynb#ch0000013?line=18'>19</a>\u001b[0m \u001b[39m    :return: The model evidence values.  \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/clinton/Documents/bse/masters_thesis/trieste/tests/scripts/loss_script.ipynb#ch0000013?line=19'>20</a>\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/clinton/Documents/bse/masters_thesis/trieste/tests/scripts/loss_script.ipynb#ch0000013?line=20'>21</a>\u001b[0m     \u001b[39mif\u001b[39;00m y_true\u001b[39m.\u001b[39;49mshape\u001b[39m.\u001b[39;49mrank \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/clinton/Documents/bse/masters_thesis/trieste/tests/scripts/loss_script.ipynb#ch0000013?line=21'>22</a>\u001b[0m         y_true \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mexpand_dims(y_true, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/clinton/Documents/bse/masters_thesis/trieste/tests/scripts/loss_script.ipynb#ch0000013?line=22'>23</a>\u001b[0m     \u001b[39mif\u001b[39;00m y_pred\u001b[39m.\u001b[39mshape\u001b[39m.\u001b[39mrank \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'rank'"
     ]
    }
   ],
   "source": [
    "y_true = tf.constant([1., 1.8])\n",
    "y_pred = tf.constant([\n",
    "                [1., 0.5, 0.4, 0.3],\n",
    "                [2., 0.3, 0.1, 0.7]\n",
    "            ])\n",
    "\n",
    "import numpy.testing as npt\n",
    "npt.assert_almost_equal(deep_evidential_regression_loss(y_true, y_pred), 0.85427415)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e644dcd3f57bf709c7b6a2361941cbaecdd3834b0d557f637bdbbd9189a2562d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.trieste': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
